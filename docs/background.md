# Background

## Generative Adversarial Network (GAN)

A GAN consists of two networks: a _generator_ network _G_ and a discriminator
network _D_ [1]. The generator _G_ takes as input a random noise __z__ sampled
from a prior distribution _p_<sub>__z__</sub> and output a fake sample
_G_(__z__). The discriminator _D_ takes as input either a sample drawn from real
data or generated by the generator and outputs a scalar indicating its
authenticity.

<img src="figs/gan.png" alt="gan" style="max-width:500px;">

The adversarial setting goes like this:

- _D tries to tell the fake samples from the real samples_
- _G tries to fool D (to make D misclassify the generated, fake samples as
  real ones)_

In general, most GAN loss functions proposed in the literature take the
following form:

<p style="padding-left:20pt">
max<sub><i>D</i></sub>
<b>E</b><sub><b>x</b>~<i>p<sub>d</sub></i></sub>
[ <i>f</i>(<i>D</i>(<b>x</b>)) ] +
<b>E</b><sub><b>x</b>~<i>p<sub>g</sub></i></sub>
[ <i>g</i>(<i>D</i>(<b>x</b>)) ]
</p>

<p style="padding-left:20pt">
min<sub><i>G</i></sub> <b>E</b><sub><b>x</b>~<i>p<sub>g</sub></i></sub>
[ <i>h</i>(<i>D</i>(<b>x</b>)) ]
</p>

Here, _p<sub>d</sub>_ denotes the _data distribution_ and _p<sub>g</sub>_
denotes the _model distribution_ implicitly defined by _G_(__z__) when
__z__~_p_<sub>__z__</sub>. Moreover, _f_, _g_ and _h_ are real functions defined
on the data space (i.e., _X_ → _R_), and we will refer to them as the
___component functions___.

## Conditional Generative Adversarial Networks (CGAN)

In a conditional GAN (CGAN) [2], both the generator _G_ and the discriminator
_D_ are now conditioned on some variable _y_. Typical (__x__, _y_) pairs include
(data, labels), (data, tags), (image, image).

![cgan](figs/cgan.png)

## Gradient penalties

As the discriminator is often found to be too strong to provide reliable
gradients to the generator, one ___regularization approach___ is to use some
gradient penalties to constrain the modeling capability of the discriminator.

Most gradient penalties proposed in the literature take the following form:

<p style="padding-left:20pt">
<i>λ</i> <b>E</b><sub><b>x</b>~<i>p<sub><b>x</b></sub></i></sub>
[ <i>R</i>( ||∇<sub><b>x</b></sub> <i>D</i>(<b>x</b>)|| ) ]
</p>

Here, the _penalty weight_ _λ_ ∈ ℝ is a pre-defined constant, and _R_(·) is a
real function. The distribution _p_<sub>__x__</sub> defines where the gradient
penalties are enforced. Note that this term will be added to the loss function
as a _regularization term_ for the discriminator.

Here are some common gradient penalties and their _p_<sub>__x__</sub> and
_R_(·).

| gradient penalty type                | _p_<sub>__x__</sub> | _R_(_x_) |
|--------------------------------------|:-------------------:|:--------:|
| coupled gradient penalties [3]       | _p<sub>d</sub>_ + _U_[0, 1] (_p<sub>g</sub>_ − _p<sub>d</sub>_) | (_x_ − _k_)<sup>2</sup> or _max_(_x_, _k_) |
| local gradient penalties [4]         | _p<sub>d</sub>_ + _c N_[0, _I_] | (_x_ − _k_)<sup>2</sup> or _max_(_x_, _k_) |
| R<sub>1</sub> gradient penalties [5] | _p<sub>d</sub>_     | _x_      |
| R<sub>2</sub> gradient penalties [5] | _p<sub>g</sub>_     | _x_      |

## Spectral normalization

Spectral normalization [6] is another ___regularization approach___ for GANs. It
normalizes the spectral norm of each layer in a neural network to enforce the
Lipschitz constraints. While the gradient penalties impose local
regularizations, the spectral normalization imposes a global regularization on
the discriminator.

## References

[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
    Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,
    "Generative Adversarial Networks,"
    in _Proc. NeurIPS_, 2014.

[2] Mehdi Mirza and Simon Osindero,
    "Conditional Generative Adversarial Nets,"
    _arXiv preprint, arXiv:1411.1784_, 2014.

[3] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
    Aaron Courville,
    "Improved Training of Wasserstein GANs,"
    in _Proc. NeurIPS_, 2017.

[4] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira,
    "On Convergence and Stability of GANs,"
    _arXiv preprint, arXiv:1705.07215_, 2017.

[5] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin,
    "Which training methods for GANs do actually converge?"
    in _Proc. ICML_, 2018.

[6] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida,
    "Spectral Normalization for Generative Adversarial Networks,"
    in _Proc. ICLR_, 2018.
