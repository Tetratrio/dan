# MNIST digit classification using Dicriminative Adversarial Networks (DANs)

## Backgrounds

### Generative Adversarial Networks (GAN)

A GAN consists of two networks: a *generator* network *G* and a discriminator
network *D* [1]. The generator *G* takes as input a random noise *__z__* sampled
from a prior distribution, *p<sub>z</sub>* and output a fake sample *G(__z__)*.
The discriminator *D* takes as input either a sample drawn from real data or
generated by the generator and try to tell the fake data from real data.

The adversarial setting goes like this:

- while *D* try to tell the fake data from real data in each run
- *G* also try to fool *D* (to make *D* misclassified the generated, fake data
  as real ones)

Theoretically, the model distribution *p(G(__z__))* will become closer to the
data distribution as the training unfolds.

<img src="figs/gan.png" alt="system" style="width:100%; max-width:400px; display:block; margin:auto">

> Formally, *G* is actually learning a projection from the prior distribution
*p(__z__)* to the data distribution *p(__x__)*. To make it clear, it's a
distribution-to-distribution mapping rather than a sample-to-sample mapping.

### Wasserstein GANs (WGAN)

The objective functions for the generator and the discrimnator in a traditional
GAN is given as:

**(Discriminator)** *max* __E__<sub>__x__~*p<sub>d</sub>*</sub>
[log(*D*(__x__))] + __E__<sub>*__z__*~*p<sub>z</sub>*</sub>
[1 - log(*D*(*G*(*__z__*)))]

**(Generator)** *min* __E__<sub>*__z__*~*p<sub>z</sub>*</sub>
[1 - log(*D*(*G*(*__z__*)))]

An alternative form called WGAN was later proposed with the intuition to
estimate the Wasserstein distance between the real and the model distributions
by a deep neural network and use it as a critic for the generator [2]. The
objective functions for WGAN can be formulated as:

**(Discriminator)** *max* __E__<sub>__x__~*p<sub>d</sub>*</sub> [*D*(__x__)]
&ndash; __E__<sub>*__z__*~*p<sub>z</sub>*</sub> [*D*(*G*(*__z__*))]
[(▽<sub>__x̄__</sub>||__x̄__|| &ndash; 1)<sup>2</sup>]

**(Generator)** *min* __E__<sub>*__z__*~*p<sub>z</sub>*</sub>
[*D*(*G*(*__z__*))]

### Wasserstein GANs with gradient penalty (WGAN-GP)

In order to enforce Lipschitz constraints on the discriminator, which is
required in the training of WGAN, Gulrajani *et al.* proposed to add to the
objective function of $D$ a gradient penalty (GP) term that punish the
discriminator when and where it violates the Lipschitz constraints [3].

However, it is computationally impossible to impose gradient penalty anywhere in
the data space. With the observation that the optimal discriminator to a fixed
generator lie in middle of the data distribution (*p<sub>d</sub>*) and the model
distribution (*p<sub>g</sub>*), they proposed to enforce gradient penalties
along straight lines between pairs of points sampled from *p<sub>d</sub>* and
*p<sub>g</sub>*. Now the objective function for the discriminator becomes

**(Discriminator)** *max* __E__<sub>__x__~*p<sub>d</sub>*</sub> [*D*(__x__)]
&ndash; __E__<sub>*__z__*~*p<sub>z</sub>*</sub> [*D*(*G*(*__z__*))] +
__E__<sub>__x̄__~*p*<sub>__x̄__</sub></sub>
[(▽<sub>__x̄__</sub>||__x̄__|| &ndash; 1)<sup>2</sup>] ,

where *p*<sub>__x̄__</sub> = *ε p<sub>d</sub>* + (1 - *ε*) *p<sub>g</sub>*,
*ε*~*U*[0, 1].

### Conditional Generative Adversarial Networks (CGAN)

In a conditional GAN (CGAN) [4], both the generator *G* and the discriminator
*D* are now conditioned on some variable *y*. Typical (*x*, *y*) pairs include
(data, label), (data, tag vector), (image, image).

<img src="figs/cgan.png" alt="system" style="width:100%; max-width:500px; display:block; margin:auto">

### Dicriminative Adversarial Networks (DAN)

The discriminative adversarial network (DAN) is proposed by Mirza *et al.* as a
discriminative framework for learning loss functions for semi-supervised
learning [5]. The generator now becomes a predictor that take as input an
unlabeled data and predict its label. The discriminator take as input either a
real-data-real-label pair (__x__, __y__) or a real-data-fake-label pair (__x__,
G(__x__)) and aim to tell the fake ones from the real ones.

Unlike traditional supervised training, where we use a surrogate loss function
as a critic for the predictor to learn the distribution *p*(__y__|__x__), in
DANs the discriminator play the role of a critic for the predictor. Moreover,
since the discriminator is also learned and optimized along the training
process, *the generator is not optimizing any specific form of surrogate loss
function*.

## MNIST digit classification using DANs

Here we give an example of using DANs for MNIST digit classification.

<img src="figs/system.png" alt="system" style="width:100%; max-width:400px; display:block; margin:auto">

## Settings

In all experiments, we use the Adam optimizer.

| parameter                | value | &emsp; | parameter       | value |
|:------------------------:|:-----:|:------:|:---------------:|:-----:|
| batch size               | 64    |        | learning rate   | 0.002 |
| number of epochs         | 20    |        | *β*<sub>1</sub> | 0.5   |
| clipping value (WGAN)    | 0.01  |        | *β*<sub>2</sub> | 0.9   |
| GP coefficient (WGAN-GP) | 10.0  |        | *ε*             | 1e-8  |

## Results

|  Model  | G (output layer) | G (hidden layers) |   D   | Accuracy   |
|:-------:|:----------------:|:-----------------:|:-----:|:----------:|
| WGAN-GP | X                | ReLU              | ReLU  | **0.9812** |
| WGAN-GP | only BN          | ReLU              | ReLU  | **0.9779** |
| WGAN-GP | softmax + BN     | LReLU             | LReLU | 0.0982     |
| WGAN-GP | softmax + BN     | LReLU             | ReLU  | *0.7899*   |
| WGAN-GP | softmax + BN     | LReLU             | LReLU | 0.0892     |
| WGAN-GP | softmax + BN     | ReLU              | ReLU  | 0.1010     |
| WGAN-GP | softmax          | LReLU             | LReLU | 0.0980     |
| WGAN-GP | softmax          | LReLU             | ReLU  | 0.1028     |
| WGAN-GP | softmax          | LReLU             | LReLU | 0.1028     |
| WGAN-GP | softmax          | ReLU              | ReLU  | *0.8913*   |
| WGAN    | softmax + BN     | LReLU             | LReLU | 0.0958     |
| WGAN    | softmax + BN     | LReLU             | ReLU  | 0.1135     |
| WGAN    | softmax + BN     | LReLU             | LReLU | 0.0098     |
| WGAN    | softmax + BN     | ReLU              | ReLU  | 0.1135     |
| WGAN    | softmax          | LReLU             | LReLU | 0.1028     |
| WGAN    | softmax          | LReLU             | ReLU  | 0.1009     |
| WGAN    | softmax          | LReLU             | LReLU | 0.1028     |
| WGAN    | softmax          | ReLU              | ReLU  | 0.1028     |

## References

[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
    Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,
    "Generative Adversarial Networks",
    in *Proc. NIPS*, 2014.

[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou,
    "Wasserstein Generative Adversarial Networks",
    in *Proc. ICML*, 2017.

[3] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
    Aaron Courville,
    "Improved Training of Wasserstein GANs",
    in *Proc. NIPS*, 2017.

[4] Mehdi Mirza and Simon Osindero,
    "Conditional Generative Adversarial Nets",
    *arXiv preprint, arXiv:1411.1784*, 2014.

[5] Cicero Nogueira dos Santos, Kahini Wadhawan, and Bowen Zhou,
    "Learning Loss Functions for Semi-supervised Learning via Discriminative
    Adversarial Networks,"
    in *NIPS Workshop on Learning with Limited Labeled Data*, 2017.
